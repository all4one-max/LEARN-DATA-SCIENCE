{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 of project: \n",
    "\n",
    "# `Printing the Decison Tree`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions used :\n",
    "\n",
    "### 1. ````DT :```` main function to print all the details\n",
    "### 2. ````find_info_gain :````  to find information gain\n",
    "### 3. ````find_split_info:````  to get split info\n",
    "### 4. ````find_best_feature :```` to find best feature to split on based on best gain ratio\n",
    "### 5. ````entropy :````  to get entropy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DT(X,Y,features,level):\n",
    "    \n",
    "    #  Get unique classes of current node\n",
    "    classes = set(Y) \n",
    "    \n",
    "    # if current node is a Pure Node\n",
    "    if(len(classes) == 1): \n",
    "        print(\"Level \",level)\n",
    "        current_class = list(classes)[0]\n",
    "        \n",
    "        print(\"Count of \",current_class,\" = \",len(Y))\n",
    "        print(\"Current Entropy is = 0.0\")\n",
    "        print(\"Reached leaf Node\")\n",
    "        print()\n",
    "        return\n",
    "    \n",
    "    # If no feature is left to split\n",
    "    elif(len(features) == 0): \n",
    "        print(\"Level \",level) \n",
    "        \n",
    "        #finding count of each output class\n",
    "        for current_class in classes:\n",
    "            count_of_current_class = (Y == current_class).sum() \n",
    "            print(\"Count of \",current_class,\" = \",count_of_current_class) \n",
    "            \n",
    "        #printing \n",
    "        entropy_current = entropy(Y)\n",
    "        \n",
    "        print(\"Current Entropy is = \",entropy_current)\n",
    "        print(\"No more features left\")\n",
    "        print(\"Reached leaf Node\")\n",
    "        print()\n",
    "        return\n",
    "    \n",
    "    #If all the features in the curr node can only take one value, then we can't split further\n",
    "    elif((find_best_feature(X,Y,features))[0] == -1):\n",
    "        print(\"Level \",level) \n",
    "        \n",
    "        #Get count of each output class\n",
    "        for i in classes:\n",
    "            j = (Y == i).sum() \n",
    "            print(\"Count of \",i,\" = \",j)\n",
    "        \n",
    "        #finding class with majority and its entropy\n",
    "        \n",
    "        entropy_current = entropy(Y)\n",
    "        \n",
    "        print(\"Current Entropy is = \",entropy_current)\n",
    "        print(\"No more features left\")\n",
    "        print(\"Reached leaf Node\")\n",
    "        print()\n",
    "        return\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"Level \",level) \n",
    "        \n",
    "        for i in classes:\n",
    "            j = (Y ==i).sum()\n",
    "            print(\"Count of \",i,\" = \",j)\n",
    "        \n",
    "        #entropy Of Current Node\n",
    "        print(\"Current Entropy is = \", entropy(Y))\n",
    "        \n",
    "        #getting best feature to split and its gain ratio\n",
    "        best_feature,gain_ratio = find_best_feature(X,Y,features)\n",
    "        print(\"Splitting on feature\",best_feature,\"with gain ratio :\",gain_ratio)\n",
    "        print()\n",
    "        \n",
    "        #Splitting curr node on all different values the best feature can have and recursively calling DT on each split\n",
    "        diff_val_of_best_feat = set(X[:,best_feature]) # Get Different values of the BEST FEATURE in current data\n",
    "        \n",
    "        #Traverse each feature value, split on each \n",
    "        for current in diff_val_of_best_feat:\n",
    "\n",
    "            x = X[(X[:,best_feature] == current)]\n",
    "            y = Y[(X[:,best_feature] == current)]\n",
    "            position_of_best_feat = np.where(features == best_feature)\n",
    "            remaining_features = np.delete(features,position_of_best_feat)\n",
    "            #Recursion\n",
    "            DT(x,y,remaining_features,level + 1)\n",
    "        \n",
    "        return \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_info_gain(X,Y,feature):\n",
    "    \n",
    "    ent = entropy(Y)\n",
    "    total_data = len(Y)\n",
    "    weighted_avg = 0\n",
    "    \n",
    "    values = set(X[:,feature])  #values the feature can take\n",
    "    \n",
    "    for i in values:\n",
    "        j = Y[(X[:,feature] == i)]\n",
    "        i_ent = entropy(j)\n",
    "      \n",
    "        weighted_avg += (i_ent*len(j))/total_data\n",
    "    \n",
    "    #finding info gain\n",
    "    info_gain = ent - weighted_avg\n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_info(X, Y,feature):\n",
    "\n",
    "    total_data = len(Y)\n",
    "    weighted_avg = 0\n",
    "    values = set(X[:,feature]) #values the feature can take\n",
    "\n",
    "    for i in values:\n",
    "        j = Y[(X[:,feature] == i)]\n",
    "        weighted_avg += ((len(j) / total_data) * np.log2((len(j) / total_data)))\n",
    "    return (-1 * weighted_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_feature(X, Y,features):\n",
    "    best_feature = -1\n",
    "    max_gain = -1\n",
    "    \n",
    "    for current_feature in features: #traversing all features\n",
    "        #finding split info and info gain\n",
    "        split_info = find_split_info(X, Y,current_feature)\n",
    "        info_gain = find_info_gain(X, Y,current_feature)       \n",
    "    \n",
    "        # finding gain ratio\n",
    "        if(split_info != 0): \n",
    "            gain_ratio = info_gain/split_info\n",
    "        else :\n",
    "            gain_ratio = -1\n",
    "        \n",
    "        # getting best feature and best gain ratio\n",
    "        if(gain_ratio>max_gain):\n",
    "            best_feature = current_feature\n",
    "            max_gain = gain_ratio\n",
    "            \n",
    "    return (best_feature,max_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(Y):\n",
    "    \n",
    "    #getting unique classes\n",
    "    classes = set(Y)\n",
    "    total_data = len(Y)\n",
    "    \n",
    "    ent = 0\n",
    "    \n",
    "    # traversing all classes and getting entropy\n",
    "    for current_class in classes:\n",
    "        \n",
    "        prob = (Y == current_class).sum()/total_data  # probability value\n",
    "        ent += (prob * np.log2(prob)) \n",
    "    return (-1 * ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and Manipulation of  Iris Data\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# To convert values of features into discrete values\n",
    "def cont_to_discrete(feature):\n",
    "    second = feature.mean()\n",
    "    first = second * 0.5\n",
    "    third = second * 1.5\n",
    "    for i in range(len(feature)):\n",
    "        if(feature[i] < first):\n",
    "            feature[i] = 0\n",
    "        elif(feature[i] < second):\n",
    "            feature[i] = 1\n",
    "        elif(feature[i] < third):\n",
    "            feature[i] = 2\n",
    "        else:\n",
    "            feature[i] = 3\n",
    "    return feature\n",
    "\n",
    "# Applying the function to all features\n",
    "for i in range(x.shape[1]):\n",
    "    cont_to_discrete(x[:,i])\n",
    "\n",
    "    \n",
    "features = np.array([i for i in range(x.shape[1])])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Output of Part 1 of Project ( Printing the Decision Tree )` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level  0\n",
      "Count of  0  =  50\n",
      "Count of  1  =  50\n",
      "Count of  2  =  50\n",
      "Current Entropy is =  1.584962500721156\n",
      "Splitting on feature 3 with gain ratio : 0.7350016280496154\n",
      "\n",
      "Level  1\n",
      "Count of  0  =  49\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level  1\n",
      "Count of  0  =  1\n",
      "Count of  1  =  10\n",
      "Current Entropy is =  0.4394969869215134\n",
      "Splitting on feature 1 with gain ratio : 1.0\n",
      "\n",
      "Level  2\n",
      "Count of  1  =  10\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level  2\n",
      "Count of  0  =  1\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level  1\n",
      "Count of  1  =  39\n",
      "Count of  2  =  5\n",
      "Current Entropy is =  0.5107878229540133\n",
      "Splitting on feature 2 with gain ratio : 0.2488471906913508\n",
      "\n",
      "Level  2\n",
      "Count of  1  =  1\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level  2\n",
      "Count of  1  =  38\n",
      "Count of  2  =  4\n",
      "Current Entropy is =  0.4537163391869448\n",
      "Splitting on feature 1 with gain ratio : 0.04070432026142338\n",
      "\n",
      "Level  3\n",
      "Count of  1  =  31\n",
      "Count of  2  =  4\n",
      "Current Entropy is =  0.512709142030877\n",
      "Splitting on feature 0 with gain ratio : 0.012981006561098147\n",
      "\n",
      "Level  4\n",
      "Count of  1  =  14\n",
      "Count of  2  =  1\n",
      "Current Entropy is =  0.35335933502142136\n",
      "No more features left\n",
      "Reached leaf Node\n",
      "\n",
      "Level  4\n",
      "Count of  1  =  17\n",
      "Count of  2  =  3\n",
      "Current Entropy is =  0.6098403047164004\n",
      "No more features left\n",
      "Reached leaf Node\n",
      "\n",
      "Level  3\n",
      "Count of  1  =  7\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level  2\n",
      "Count of  2  =  1\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level  1\n",
      "Count of  1  =  1\n",
      "Count of  2  =  45\n",
      "Current Entropy is =  0.15109697051711368\n",
      "Splitting on feature 1 with gain ratio : 0.031037861792700953\n",
      "\n",
      "Level  2\n",
      "Count of  2  =  28\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n",
      "Level  2\n",
      "Count of  1  =  1\n",
      "Count of  2  =  17\n",
      "Current Entropy is =  0.3095434291503252\n",
      "Splitting on feature 2 with gain ratio : 0.057914261762502306\n",
      "\n",
      "Level  3\n",
      "Count of  1  =  1\n",
      "Count of  2  =  8\n",
      "Current Entropy is =  0.5032583347756457\n",
      "No more features left\n",
      "Reached leaf Node\n",
      "\n",
      "Level  3\n",
      "Count of  2  =  9\n",
      "Current Entropy is = 0.0\n",
      "Reached leaf Node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calling the function to print our Decision Tree\n",
    "DT(x,y,features,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 of Project\n",
    "\n",
    "# `BUILDING THE DECISION TREE`\n",
    "\n",
    "## functions used:\n",
    "\n",
    "### 1. `findMaxClass :` to find class with majority of data\n",
    "### 2. `buildDT :` to build the decision tree and return the root of the resultant Decision Tree\n",
    "### 3. `printTree :` to print the details of all Nodes given a Root Node of a Decision Tree\n",
    "\n",
    "## classes used:\n",
    "\n",
    "### `Node :` this class defines a node with properties given  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function would find the class with maximum count\n",
    "def findMaxClass(Y):\n",
    "    classes = set(Y) \n",
    " \n",
    "    best_class  = None\n",
    "    count  = -1\n",
    "    \n",
    "    for i in classes:\n",
    "        \n",
    "        j = (Y == i).sum()\n",
    "        \n",
    "        if(j > count):\n",
    "            count = j\n",
    "            best_class = i\n",
    "            \n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDT(X,Y,features):\n",
    "   \n",
    "    classes = set(Y) \n",
    "    \n",
    "    # if current node is a Pure Node\n",
    "    if(len(classes) == 1):\n",
    "        leafNode = Node()\n",
    "        leafNode.entropy = 0\n",
    "        leafNode.data = len(Y)\n",
    "        leafNode.prediction = Y[0]\n",
    "        leafNode.children = list()\n",
    "        leafNode.class_split[Y[0]] = len(Y)\n",
    "        leafNode.current_prediction = Y[0]\n",
    "        leafNode.split_feature = None\n",
    "        return leafNode\n",
    "    \n",
    "    # If no feature is left to split\n",
    "    elif(len(features) == 0):\n",
    "        \n",
    "        leafNode = Node()\n",
    "        leafNode.entropy = entropy(Y)\n",
    "        leafNode.data = len(Y)\n",
    "        leafNode.children = list()\n",
    "        leafNode.split_feature = None\n",
    "        \n",
    "       #finding class with maximum number of data points\n",
    "        leafNode.current_prediction = findMaxClass(Y)\n",
    "        \n",
    "        for i in classes:\n",
    "            j = (Y == i).sum() \n",
    "            leafNode.class_split[i] = j\n",
    "        \n",
    "        #majority_class = findMajorityClass(Y_data)#Get class with max num of data points\n",
    "        #leafNode.current_prediction = majority_class\n",
    "        \n",
    "        return leafNode\n",
    "    \n",
    "    #If all the features in the curr node can only take one value, then we can't split further\n",
    "    elif((find_best_feature(X,Y,features))[0] == -1):\n",
    "        \n",
    "        leafNode = Node()\n",
    "        leafNode.entropy = entropy(Y)\n",
    "        leafNode.data = len(Y)\n",
    "        leafNode.children = list()\n",
    "        leafNode.split_feature = None\n",
    "        leafNode.current_prediction = findMaxClass(Y) \n",
    "        \n",
    "        for i in classes:\n",
    "            j= (Y == i).sum() \n",
    "            leafNode.class_split[i] = j\n",
    "        \n",
    "        #majority_class = findMajorityClass(Y_data)#Get class with max num of data points\n",
    "        #leafNode.current_prediction = majority_class\n",
    "        \n",
    "        return leafNode\n",
    "    \n",
    "    else:\n",
    "        internalNode = Node()\n",
    "        internalNode.entropy = entropy(Y)\n",
    "        internalNode.data = len(Y)\n",
    "        internalNode.current_prediction = findMaxClass(Y)\n",
    "        \n",
    "        \n",
    "        #getting best feature to split and gain ratio\n",
    "        best_feature,gain_ratio = find_best_feature(X,Y,features)\n",
    "        internalNode.split_feature = best_feature\n",
    "        internalNode.gain_ratio = gain_ratio \n",
    "        \n",
    "        for i in classes:\n",
    "            j = (Y == i).sum()\n",
    "            internalNode.class_split[i] = j\n",
    "            \n",
    "        \n",
    "        #Splitting curr node on all different values the best feature can have and recursively calling DT on each split\n",
    "        diff_val_of_best_feat = set(X[:,best_feature]) # Get Different values of the BEST FEATURE in current data\n",
    "        \n",
    "        #Traverse each feature value, split on each \n",
    "        for current in diff_val_of_best_feat:\n",
    "            x = X[(X[:,best_feature] == current)]\n",
    "            y = Y[(X[:,best_feature] == current)]\n",
    "            \n",
    "            remaining_features = np.delete(features,np.where(features == best_feature))\n",
    "            \n",
    "            child = buildDT(x,y,remaining_features)#Recursive Call\n",
    "            \n",
    "            internalNode.children.append(child)\n",
    "            \n",
    "        return internalNode    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    \n",
    "    # INFO ABOUT PROPERTIES:\n",
    "    # entropy: entropy of node\n",
    "    # data: total number of data points in current node\n",
    "    # class_split: dictionary having classes as keys and the number data points of the class as value\n",
    "    # children: contails the children of current node \n",
    "    # split_feature: the feature we are splitting on ( for internal nodes )\n",
    "    # gain_ratio: gain ratio taken while splitting\n",
    "    \n",
    "    def __init__(self,entropy=0,data=0,class_split = dict(),children = list(),current_prediction=-1,split_feature = None,gain_ratio = None):\n",
    "        self.data = data\n",
    "        self.entropy = entropy\n",
    "        self.class_split = dict() \n",
    "        self.children = list()\n",
    "        self.current_prediction = current_prediction\n",
    "        self.split_feature = None\n",
    "        self.gain_ratio = None\n",
    "        return \n",
    "    \n",
    "    def addChild(self, c):\n",
    "        self.children.append(c)\n",
    "    \n",
    "    def getDetailsNode(self,feature_list,target):\n",
    "        print(\"Samples : \",self.data)\n",
    "        print(\"Entropy : \",self.entropy)\n",
    "        print(\"Current Prediction : \",target[self.current_prediction])\n",
    "        \n",
    "        classes = self.class_split.keys()\n",
    "        for curr_class in classes:\n",
    "            print(\"Class \",target[curr_class],\" has Count\",self.class_split[curr_class] )\n",
    "            \n",
    "        if(self.split_feature != None):\n",
    "            print(\"Splitting on feature :\", feature_list[self.split_feature],\", with gain ratio : \",self.gain_ratio)\n",
    "        else:\n",
    "            print(\"Leaf Node\")\n",
    "            \n",
    "        print()\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTree(root,feature_list,target):\n",
    "    # if node is a leaf node\n",
    "    if(len(root.children) == 0):\n",
    "        root.getDetailsNode(feature_list,target)\n",
    "        return\n",
    "    \n",
    "    # printing details of root Node\n",
    "    root.getDetailsNode(feature_list,target)\n",
    "    \n",
    "    # printing details of children of root Node recursively\n",
    "    for current_child in root.children:\n",
    "        printTree(current_child,feature_list,target)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Output of PArt 2 of Project ( Building the Decision Tree )` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples :  150\n",
      "Entropy :  1.584962500721156\n",
      "Current Prediction :  setosa\n",
      "Class  setosa  has Count 50\n",
      "Class  versicolor  has Count 50\n",
      "Class  virginica  has Count 50\n",
      "Splitting on feature : petal width (cm) , with gain ratio :  0.7350016280496154\n",
      "\n",
      "Samples :  49\n",
      "Entropy :  0\n",
      "Current Prediction :  setosa\n",
      "Class  setosa  has Count 49\n",
      "Leaf Node\n",
      "\n",
      "Samples :  11\n",
      "Entropy :  0.4394969869215134\n",
      "Current Prediction :  versicolor\n",
      "Class  setosa  has Count 1\n",
      "Class  versicolor  has Count 10\n",
      "Splitting on feature : sepal width (cm) , with gain ratio :  1.0\n",
      "\n",
      "Samples :  10\n",
      "Entropy :  0\n",
      "Current Prediction :  versicolor\n",
      "Class  versicolor  has Count 10\n",
      "Leaf Node\n",
      "\n",
      "Samples :  1\n",
      "Entropy :  0\n",
      "Current Prediction :  setosa\n",
      "Class  setosa  has Count 1\n",
      "Leaf Node\n",
      "\n",
      "Samples :  44\n",
      "Entropy :  0.5107878229540133\n",
      "Current Prediction :  versicolor\n",
      "Class  versicolor  has Count 39\n",
      "Class  virginica  has Count 5\n",
      "Splitting on feature : petal length (cm) , with gain ratio :  0.2488471906913508\n",
      "\n",
      "Samples :  1\n",
      "Entropy :  0\n",
      "Current Prediction :  versicolor\n",
      "Class  versicolor  has Count 1\n",
      "Leaf Node\n",
      "\n",
      "Samples :  42\n",
      "Entropy :  0.4537163391869448\n",
      "Current Prediction :  versicolor\n",
      "Class  versicolor  has Count 38\n",
      "Class  virginica  has Count 4\n",
      "Splitting on feature : sepal width (cm) , with gain ratio :  0.04070432026142338\n",
      "\n",
      "Samples :  35\n",
      "Entropy :  0.512709142030877\n",
      "Current Prediction :  versicolor\n",
      "Class  versicolor  has Count 31\n",
      "Class  virginica  has Count 4\n",
      "Splitting on feature : sepal length (cm) , with gain ratio :  0.012981006561098147\n",
      "\n",
      "Samples :  15\n",
      "Entropy :  0.35335933502142136\n",
      "Current Prediction :  versicolor\n",
      "Class  versicolor  has Count 14\n",
      "Class  virginica  has Count 1\n",
      "Leaf Node\n",
      "\n",
      "Samples :  20\n",
      "Entropy :  0.6098403047164004\n",
      "Current Prediction :  versicolor\n",
      "Class  versicolor  has Count 17\n",
      "Class  virginica  has Count 3\n",
      "Leaf Node\n",
      "\n",
      "Samples :  7\n",
      "Entropy :  0\n",
      "Current Prediction :  versicolor\n",
      "Class  versicolor  has Count 7\n",
      "Leaf Node\n",
      "\n",
      "Samples :  1\n",
      "Entropy :  0\n",
      "Current Prediction :  virginica\n",
      "Class  virginica  has Count 1\n",
      "Leaf Node\n",
      "\n",
      "Samples :  46\n",
      "Entropy :  0.15109697051711368\n",
      "Current Prediction :  virginica\n",
      "Class  versicolor  has Count 1\n",
      "Class  virginica  has Count 45\n",
      "Splitting on feature : sepal width (cm) , with gain ratio :  0.031037861792700953\n",
      "\n",
      "Samples :  28\n",
      "Entropy :  0\n",
      "Current Prediction :  virginica\n",
      "Class  virginica  has Count 28\n",
      "Leaf Node\n",
      "\n",
      "Samples :  18\n",
      "Entropy :  0.3095434291503252\n",
      "Current Prediction :  virginica\n",
      "Class  versicolor  has Count 1\n",
      "Class  virginica  has Count 17\n",
      "Splitting on feature : petal length (cm) , with gain ratio :  0.057914261762502306\n",
      "\n",
      "Samples :  9\n",
      "Entropy :  0.5032583347756457\n",
      "Current Prediction :  virginica\n",
      "Class  versicolor  has Count 1\n",
      "Class  virginica  has Count 8\n",
      "Leaf Node\n",
      "\n",
      "Samples :  9\n",
      "Entropy :  0\n",
      "Current Prediction :  virginica\n",
      "Class  virginica  has Count 9\n",
      "Leaf Node\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Getting the root of the built tree and further printing the details of that Decision Tree from its Root Node \n",
    "\n",
    "root_for_iris_dataset = buildDT(x,y,features)\n",
    "printTree(root_for_iris_dataset,iris.feature_names,iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Submitted by:\n",
    "### ` Ritambhra Vatsya` vritambhara@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
