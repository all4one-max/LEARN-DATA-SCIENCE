{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN stands for K-Nearest Neighbours. KNN is simple classification algorithm and it is generally used for datasets in which data points are separated into several classes and we have to predict the class for the new sample point.\n",
    "KNN is <b>non-parametric</b> and <b>lazy</b> learning algorithm.\n",
    "\n",
    "<b>Non-parametric</b> basically means that the algorithm does not make any assumptions on the given data distribution. NON-parametric covers technique that do not rely on data belonging to particular distribution and do not assume the structure of model to be fixed. So, KNN is used as classification algorithm in cases where we do not have much information about the distribution of the data.\n",
    "\n",
    "KNN is referred to as <b>Lazy</b> algorithm since is does not uses training points to do any generalization, which means that there is no separate training phase. KNN keeps all the training data and uses most of the training data during the testing phase. Thus, KNN does not learn any model, it make predictions on the fly, computing similarity between testing point and each training data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN algorithm is  based on <b>feature similarity</b>. We can classify the testing data point on the basis of resemblance of its features with that of the training data set.\n",
    "\n",
    "\n",
    "![title](knn_intro.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test sample(green circle) should either be classified into Class 1(blue squares) or Class 2(red triangles).\n",
    "The class for the sample testing point is decided on the basis of majority vote-out.\n",
    "\n",
    "If value of K=1, the nearest training point belongs to Class 1, so we will say that the testing sample belongs to Class 1. Now, take the value of K to be 3, again using the methodology of majority vote, we will say that testing sample belongs to Class 2(red traingles), since out of three nearest training data points, two belongs to Class 2 and one belong to Class 1.\n",
    "\n",
    "Hence, using the same method, we can predict the class for the tesing smaple, using pre-decided value of K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN can be used for <b>Classification</b> as well as <b>Regression</b>.\n",
    "\n",
    "While using KNN for classification - output is a class membership, we will classify the sample point using the technique of majority vote among its neighbors and the most common class among its k nearest neighbours is assigned to the testing point. In regression, output is the property value of object, this value is average or median of the value of its K nearest neighbors.\n",
    "\n",
    "In classification, KNN is used to predict a class which is a discrete value whereas in regression, KNN predicts continuous values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why don't we choose value value of K=1 ?\n",
    "Choosing the value of K=1 makes our model more prone to outliers and overfitting. Value of K=1 means that we will consider only the closest (or nearest) neighbor to predict the class for our testing sample, and in majority of the cases it will lead to overfitting.\n",
    "\n",
    "![title](knn_k1.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case here, if we choose value of K=1, the given training sample will get classified as dot(Class 2), instead of being classified as a cross(Class 1). So, in such cases certain optimal value of K should be choosen to get good results. Choosing the value of K = 1, leads to formation of complex decision boundaries and hence will lead to overfitting to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be using the majority vote technique, so value of K is taken to be odd, to obtain clear result about the class of the testing data sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Metric for KNN\n",
    "\n",
    "There are various distance metrices that can be choosen are Manhattan Distance, Euclidian Distance, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\textbf{MANHATTAN DISTANCE} = \\left| \\sum_{i=1}^n X_1 ^ i - X_2 ^ i \\right|\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\textbf{EUCLIDIAN DISTANCE} = \\sqrt{ \\sum_{i=1}^n \\left( X_1 ^ i - X_2 ^ i \\right) ^ 2 }\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where X<sub>1</sub> and X<sub>2</sub> are two different data points and 'i' traverse over all the features in the given dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations in KNN\n",
    "\n",
    "Variations in KNN are possible on the basis of how neighboring points are going to vote. The weight of the vote of the testing point(s) is inversely proportional to its distance from the testing point. We can go either with uniform voting or with weighted voting. In case of weighted voting, the point nearer to the testing sample will have a larger say in the vote as compared to the point which is farther away from the testing point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling before KNN\n",
    "\n",
    "In KNN, we are looking for points which are closest to the testing point. In case we do not perform feature scaling, if we have value of one feature in thousands and value of other feature in smaller units, then the effect of first feature will completely overpower and dominate over the effect of second feature in the final output. So, it is of utmost importance to apply feature scaling before applying KNN so that all the features have equal contribution in the final predicted output for the given testing point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN in Sklearn\n",
    "\n",
    "We will be applying KNN on Breast Cancer Dataset and use inbuilt KNN Classifier inside sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error', 'fractal dimension error',\n",
       "       'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
       "       'worst smoothness', 'worst compactness', 'worst concavity',\n",
       "       'worst concave points', 'worst symmetry', 'worst fractal dimension'],\n",
       "      dtype='<U23')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93859649122807021"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default value of K (number of neighbors) is equal to 5 in Sklearn. By default, Sklearn implements <b>Minkowski</b> distance metric. General form of Minkowski distance is :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\textbf{MINKOWSKI DISTANCE} = \\left| \\sum_{i=1}^n X_1 ^ i - X_2 ^ i \\right| ^ p\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If p = 1, that means we are using Manhattan Distance and if p = 2, that means we are using Euclidian Distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best value of K is the one for which we get lowest error on testing data. So, what we can do is to repeatedly train our model using both training and testing data, for different values of parameter K and then finally choose value of K which results in minimum error. But in this process, we are using the testing data as a part of our training process to obtain the optimal value of K. Hence, this process is not to be used. On the other hand if we use only training data and tune the value of parameter K, it will lead to overfitting. This will result in lower value of error on training data, but comparatively higher value of error on the testing data.\n",
    "\n",
    "So, to obtain the optimal value of K, we use a method known as CROSS VALIDATION. Cross Validation basically means taking out the subset from the training data and not using this subset in the training process. This subset of training data is called the 'validation set'. There are various techniques available for cross validation, we will be using the most general one, known as <b>K-fold cross validation</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](knn_crossval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In K fold cross validation, the training data is randomly split into K different samples(or folds). One of the sample is taken to be the validation set and the model is fitted on the remaining K-1 samples. The accuracy of the model is then computed. The same process is repeated K times, each time taking a different sample of points to be in the validation set. This results in K value for test error and these values are averaged out to obtain the overall result.\n",
    "\n",
    "Cross Validation is used to estimate the test error and generate more robust models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation in Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  1\n",
      "Scores : \n",
      "[ 0.93478261  0.95652174  0.91304348  0.84782609  0.86956522  0.93333333\n",
      "  0.97777778  0.91111111  0.93333333  0.93333333]\n",
      "Mean Score =  0.921062801932\n",
      "\n",
      "K =  3\n",
      "Scores : \n",
      "[ 0.91304348  0.95652174  0.86956522  0.93478261  0.89130435  0.95555556\n",
      "  0.95555556  0.88888889  0.95555556  0.95555556]\n",
      "Mean Score =  0.927632850242\n",
      "\n",
      "K =  5\n",
      "Scores : \n",
      "[ 0.95652174  0.97826087  0.86956522  0.91304348  0.91304348  0.93333333\n",
      "  0.97777778  0.86666667  0.95555556  0.91111111]\n",
      "Mean Score =  0.927487922705\n",
      "\n",
      "K =  7\n",
      "Scores : \n",
      "[ 0.95652174  0.95652174  0.89130435  0.91304348  0.89130435  0.97777778\n",
      "  0.97777778  0.86666667  0.95555556  0.93333333]\n",
      "Mean Score =  0.931980676329\n",
      "\n",
      "K =  9\n",
      "Scores : \n",
      "[ 0.95652174  0.95652174  0.89130435  0.91304348  0.86956522  0.97777778\n",
      "  0.97777778  0.86666667  0.95555556  0.95555556]\n",
      "Mean Score =  0.932028985507\n",
      "\n",
      "K =  11\n",
      "Scores : \n",
      "[ 0.95652174  0.95652174  0.91304348  0.91304348  0.89130435  0.97777778\n",
      "  0.97777778  0.86666667  0.93333333  0.93333333]\n",
      "Mean Score =  0.93193236715\n",
      "\n",
      "K =  13\n",
      "Scores : \n",
      "[ 0.95652174  0.95652174  0.89130435  0.91304348  0.89130435  0.97777778\n",
      "  0.97777778  0.86666667  0.95555556  0.93333333]\n",
      "Mean Score =  0.931980676329\n",
      "\n",
      "K =  15\n",
      "Scores : \n",
      "[ 0.95652174  0.93478261  0.86956522  0.91304348  0.89130435  0.97777778\n",
      "  0.97777778  0.88888889  0.93333333  0.91111111]\n",
      "Mean Score =  0.925410628019\n",
      "\n",
      "K =  17\n",
      "Scores : \n",
      "[ 0.95652174  0.93478261  0.86956522  0.93478261  0.89130435  0.97777778\n",
      "  0.97777778  0.88888889  0.93333333  0.91111111]\n",
      "Mean Score =  0.927584541063\n",
      "\n",
      "K =  19\n",
      "Scores : \n",
      "[ 0.95652174  0.93478261  0.86956522  0.91304348  0.86956522  0.97777778\n",
      "  0.97777778  0.88888889  0.93333333  0.91111111]\n",
      "Mean Score =  0.923236714976\n",
      "\n",
      "K =  21\n",
      "Scores : \n",
      "[ 0.95652174  0.93478261  0.86956522  0.91304348  0.86956522  0.97777778\n",
      "  0.97777778  0.86666667  0.93333333  0.91111111]\n",
      "Mean Score =  0.921014492754\n",
      "\n",
      "K =  23\n",
      "Scores : \n",
      "[ 0.95652174  0.93478261  0.86956522  0.91304348  0.86956522  0.97777778\n",
      "  0.97777778  0.84444444  0.93333333  0.91111111]\n",
      "Mean Score =  0.918792270531\n",
      "\n",
      "K =  25\n",
      "Scores : \n",
      "[ 0.95652174  0.93478261  0.86956522  0.91304348  0.84782609  0.97777778\n",
      "  0.97777778  0.84444444  0.93333333  0.91111111]\n",
      "Mean Score =  0.916618357488\n",
      "\n",
      "K =  27\n",
      "Scores : \n",
      "[ 0.95652174  0.93478261  0.89130435  0.91304348  0.84782609  0.95555556\n",
      "  0.97777778  0.84444444  0.93333333  0.91111111]\n",
      "Mean Score =  0.916570048309\n",
      "\n",
      "K =  29\n",
      "Scores : \n",
      "[ 0.95652174  0.93478261  0.89130435  0.91304348  0.84782609  0.95555556\n",
      "  0.97777778  0.84444444  0.93333333  0.91111111]\n",
      "Mean Score =  0.916570048309\n",
      "\n",
      "K =  31\n",
      "Scores : \n",
      "[ 0.93478261  0.93478261  0.89130435  0.91304348  0.84782609  0.95555556\n",
      "  0.97777778  0.84444444  0.93333333  0.91111111]\n",
      "Mean Score =  0.914396135266\n",
      "\n",
      "K =  33\n",
      "Scores : \n",
      "[ 0.93478261  0.93478261  0.89130435  0.91304348  0.84782609  0.95555556\n",
      "  0.97777778  0.84444444  0.93333333  0.91111111]\n",
      "Mean Score =  0.914396135266\n",
      "\n",
      "K =  35\n",
      "Scores : \n",
      "[ 0.95652174  0.93478261  0.89130435  0.91304348  0.84782609  0.95555556\n",
      "  0.97777778  0.86666667  0.93333333  0.91111111]\n",
      "Mean Score =  0.918792270531\n",
      "\n",
      "K =  37\n",
      "Scores : \n",
      "[ 0.93478261  0.93478261  0.91304348  0.91304348  0.84782609  0.95555556\n",
      "  0.95555556  0.84444444  0.93333333  0.91111111]\n",
      "Mean Score =  0.914347826087\n",
      "\n",
      "K =  39\n",
      "Scores : \n",
      "[ 0.93478261  0.93478261  0.89130435  0.89130435  0.84782609  0.95555556\n",
      "  0.95555556  0.86666667  0.93333333  0.91111111]\n",
      "Mean Score =  0.912222222222\n",
      "\n",
      "K =  41\n",
      "Scores : \n",
      "[ 0.93478261  0.91304348  0.91304348  0.86956522  0.86956522  0.95555556\n",
      "  0.95555556  0.86666667  0.93333333  0.91111111]\n",
      "Mean Score =  0.912222222222\n",
      "\n",
      "K =  43\n",
      "Scores : \n",
      "[ 0.93478261  0.91304348  0.91304348  0.86956522  0.84782609  0.95555556\n",
      "  0.95555556  0.86666667  0.93333333  0.91111111]\n",
      "Mean Score =  0.910048309179\n",
      "\n",
      "K =  45\n",
      "Scores : \n",
      "[ 0.91304348  0.91304348  0.91304348  0.86956522  0.84782609  0.95555556\n",
      "  0.95555556  0.86666667  0.93333333  0.91111111]\n",
      "Mean Score =  0.907874396135\n",
      "\n",
      "K =  47\n",
      "Scores : \n",
      "[ 0.91304348  0.91304348  0.93478261  0.89130435  0.84782609  0.95555556\n",
      "  0.95555556  0.86666667  0.93333333  0.91111111]\n",
      "Mean Score =  0.912222222222\n",
      "\n",
      "K =  49\n",
      "Scores : \n",
      "[ 0.91304348  0.93478261  0.93478261  0.89130435  0.84782609  0.95555556\n",
      "  0.95555556  0.86666667  0.93333333  0.91111111]\n",
      "Mean Score =  0.914396135266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_axis = []\n",
    "k_scores = []\n",
    "for k in range(1,50, 2):\n",
    "    x_axis.append(k)\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(clf, x_train, y_train, cv=10, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "    \n",
    "    #Printing values\n",
    "    print(\"K = \",k)\n",
    "    print(\"Scores : \")\n",
    "    print(scores)\n",
    "    print(\"Mean Score = \",scores.mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngP8P+py91Sfs7SEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f64064f1b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_axis, k_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_k = x_axis[k_scores.index(max(k_scores))]\n",
    "optimal_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this way, we can choose the optimal value of K using cross validation. If the value of K is very less, for eg, say K=1, it will lead to overfitting and result in formation of complex decision boundaries. On the the hand, if the value of K is very high, we are basically underfitting and not actually taking class of neighbors into consideration. In this case, we predict the class of the testing sample from the majority class of the overall training data rather than from the majority class given by the neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower values of K corresponds to low bias but high variance and result in jagged and complex decision boundaries, while higher value of K corresponds to lower variance but increased bias and leads to formation of smoother decision boundaries.\n",
    "\n",
    "K = 1             |   K = 20\n",
    ":-------------------------:|:-------------------------:\n",
    "![](knn_dec1.png)  |  ![](knn_dec20.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(cancer.data, cancer.target, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors = 7)\n",
    "clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94736842105263153"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x,y):\n",
    "    return\n",
    "\n",
    "def predict_one(x_train, y_train, x_test, k):\n",
    "    distances = []\n",
    "    for i in range(len(x_train)):\n",
    "        distance = ((x_train[i,:] - x_test)**2).sum()\n",
    "        distances.append([distance,i])\n",
    "    distances = sorted(distances)\n",
    "    targets = []\n",
    "    for i in range(k):\n",
    "        index_of_training_data = distances[i][1]\n",
    "        targets.append(y_train[index_of_training_data])\n",
    "    return Counter(targets).most_common(1)[0][0]\n",
    "    \n",
    "def predict(x_train, y_train, x_test_data, k):\n",
    "    predictions = []\n",
    "    for x_test in x_test_data:\n",
    "        predictions.append(predict_one(x_train, y_train, x_test, k))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94736842105263153"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(X_train, Y_train, X_test, 7)\n",
    "accuracy_score(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of Dimensionality\n",
    "\n",
    "In case of KNN, if our data has more irrelevant features, then it will lead to bad results. Also, if we have two correlated features, then both of these take part in the calculations and net effect due to addition of these becomes double, and we might up getting wrong results.\n",
    "\n",
    "We can do following things to solve this problem :\n",
    "\n",
    "1.<b>Assign weights to features</b>\n",
    "\n",
    "\\begin{equation*}\n",
    " \\sum_{i=1}^n W_i \\times \\left( X_1 ^ i - X_2 ^ i \\right) ^ 2\n",
    "\\end{equation*}\n",
    "\n",
    "This can be used as the distance metric, to calculate distance between X<sub>1</sub> and X<sub>2</sub>, where W<sub>i</sub> is the weight assigned to the ith feature. These weights can be choosen randomly or can be calculated using Gradient Descent and deciding upon the cost function which is to be minimised and hence, obtain these weights.\n",
    "\n",
    "This proves to be benificial in the cases where two features are highly correlated, in that case either both these features are going to get high weights or one is going to get high weight and other is going to get the lower weight. And, therefore, we can obtain better results using this technique. \n",
    "\n",
    "2.<b>Feature Selection</b>\n",
    "\n",
    "We will apply feature selection before using KNN Classifier. <b>Backward Elimination</b> is the technique used doing feature selection, using which we will keep few features and get rid of other features from all the features intially in the dataset. In this technique, we will traverse over all the features and obtain the accuracy score by keeping this feature and next time, by removing this feature. If the accuracy of the model improves on removing this feature, then we will exclude this feature, otherwise we will keep this feature and likewise do the same thing for all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data For KNN\n",
    "\n",
    "* ##### Rescale Data\n",
    "KNN performs much better if data points have the same scale. Its a good practice to normalize data in [0,1] or according to Gaussian Distribution to obtain good results.\n",
    "\n",
    "* ##### Fill Missing Data Values\n",
    "Missing data implies that for those points distance cannot be calculated, therefore these data points could be ignored or filled with some value to compute distance.\n",
    "\n",
    "* ##### Lower Dimensionality\n",
    "KNN works pretty well for lower dimension data. Therefore, any of the above mentioned techniques can be used to lower the dimensionality of data before applying KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of labelled data, if the data is in Binary Values i.e. having one two possible values, say Yes/No or A/B, then the values are simply replaced by 0/1.\n",
    "\n",
    "If the labels are in some natural order, say we have Low/Mid/High, then we can simply assign values such as :<br/>\n",
    "Low  - 0 <br/>\n",
    "Mid  - 1 <br/>\n",
    "High - 2 <br/>\n",
    "\n",
    "Now lets suppose we have three possible data values, say Red, Blue and Green. In this case, if we assign values such as : <br/>\n",
    "Red   - 0 <br/>\n",
    "Blue  - 1 <br/>\n",
    "Green - 2 <br/>\n",
    "\n",
    "This means that the two data points having colors Red and Blue ( \\sqrt{(1-0)^2} = 1 ) are closer to each other as compared to other two data points having colors Red and Green ( \\sqrt{(2-0)^2} = 2 ). And this makes no sense, to assign values like this for such type of labelled data. \n",
    "To handle this problem, we use ONE HOT ENCODING, in which we convert these values into binary data, with number of columns equal to number of different labels in the data. Like in this case, we make three different columns, for Red, BLue and Green, each of these will now have values either 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Algorithms for KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KDTree and BallTree can be used to improve the performance the model. The basic intuition behind these is to build something like Binary Search Tree using the training data and then for the testing data traverse through the whole tree and on reaching the leaf node, obtain the K Nearest Neighbors from all the nodes traversed by the testing sample point. For more details about these algorithms, refer to the documentation link http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html.\n",
    "\n",
    "KDTree, BallTree, brute force method and other algorithms can be used depending upon the data. Applying various of these methods, we can see the results and compare their performance with different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROS and CONS of KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PROS :\n",
    "\n",
    "* Easy to understand and code.\n",
    "* Works well for multi-class classification.\n",
    "* Insensitive to outliers for optimal choice for value of K, but accuracy can be affected from noise and irrelevant features.\n",
    "* Versatile i.e. can work for both classification and regression.\n",
    "\n",
    "#### CONS :\n",
    "* Computationally expensive, because the algorithm stores all the training data\n",
    "* Testing time is huge.\n",
    "* Sensitive to irrelevant features and scaling.\n",
    "* If training data split is biased i.e. majority of data points belong to particular class, then KNN also gets biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
